import asyncio
from typing import List, Any, AsyncGenerator, Dict, Optional
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_core import CancellationToken
from autogen_agentchat.base import Response
from .tool_parser import ToolCallParser, ToolCall

class PromptBasedAgent:
    """í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ íˆ´ í˜¸ì¶œì„ ì²˜ë¦¬í•˜ëŠ” ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸"""

    def __init__(self, model_client, tools: List[Any], system_message: str, max_tool_iterations: int = 5):
        self.model_client = model_client
        self.tools = {tool.name: tool for tool in tools} if tools else {}
        self.system_message = system_message
        self.max_tool_iterations = max_tool_iterations
        self.conversation_history = []

    async def run(self, task: str) -> Response:
        """ê¸°ë³¸ ì‹¤í–‰ ë©”ì„œë“œ (AutoGen í˜¸í™˜ì„±)"""
        messages = []
        async for message in self.run_stream(task=task):
            messages.append(message)

        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ chat_messageë¡œ, ë‚˜ë¨¸ì§€ë¥¼ inner_messagesë¡œ ì‚¬ìš©
        if messages:
            return Response(chat_message=messages[-1], inner_messages=messages[:-1] if len(messages) > 1 else None)
        else:
            # ë¹ˆ ì‘ë‹µì˜ ê²½ìš°
            empty_message = TextMessage(content="No response generated.", source="assistant")
            return Response(chat_message=empty_message)

    async def run_stream(self, task: str) -> AsyncGenerator[ChatMessage, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        input_message = TextMessage(content=task, source="user")
        async for message in self.on_messages_stream([input_message], CancellationToken()):
            yield message

    async def on_messages_stream(self, messages: List[ChatMessage], cancellation_token: CancellationToken) -> AsyncGenerator[ChatMessage, None]:
        """ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ (AutoGen í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤)"""
        # ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        for msg in messages:
            if hasattr(msg, 'content') and hasattr(msg, 'source'):
                self.conversation_history.append({"role": msg.source, "content": msg.content})

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨í•œ ì „ì²´ ëŒ€í™” êµ¬ì„±
        full_conversation = [{"role": "system", "content": self.system_message}]
        full_conversation.extend(self.conversation_history)

        # íˆ´ í˜¸ì¶œ ë°˜ë³µ ì²˜ë¦¬
        iteration = 0
        while iteration < self.max_tool_iterations:
            if cancellation_token.is_cancelled():
                break

            # LLM í˜¸ì¶œ
            try:
                response = await self._call_llm(full_conversation)
                if not response:
                    break

                # íˆ´ í˜¸ì¶œ íŒŒì‹±
                tool_calls = ToolCallParser.parse_tool_calls(response)

                if not tool_calls:
                    # íˆ´ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ìµœì¢… ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬
                    clean_response = ToolCallParser.remove_tool_calls_from_response(response)
                    if clean_response.strip():
                        yield TextMessage(content=clean_response, source="assistant")
                    break

                # íˆ´ í˜¸ì¶œ ì‹¤í–‰
                tool_results = []
                for tool_call in tool_calls:
                    try:
                        result = await self._execute_tool(tool_call)
                        tool_results.append(ToolCallParser.format_tool_result(tool_call.name, result))

                        # íˆ´ ì‹¤í–‰ ìƒíƒœë¥¼ ìŠ¤íŠ¸ë¦¬ë°
                        status_msg = f"ğŸ›  ë„êµ¬ '{tool_call.name}' ì‹¤í–‰ ì™„ë£Œ"
                        yield TextMessage(content=status_msg, source="assistant")

                    except Exception as e:
                        error_result = f"ì˜¤ë¥˜: {str(e)}"
                        tool_results.append(ToolCallParser.format_tool_result(tool_call.name, error_result))

                # íˆ´ ê²°ê³¼ë¥¼ ëŒ€í™”ì— ì¶”ê°€
                if tool_results:
                    tool_response = response + "\n" + "\n".join(tool_results)
                    full_conversation.append({"role": "assistant", "content": tool_response})

                iteration += 1

            except Exception as e:
                error_msg = f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}"
                yield TextMessage(content=error_msg, source="assistant")
                break

        # ë§ˆì§€ë§‰ ëŒ€í™”ë¥¼ íˆìŠ¤í† ë¦¬ì— ì €ì¥
        if full_conversation and len(full_conversation) > len(self.conversation_history) + 1:
            self.conversation_history.append({
                "role": "assistant",
                "content": full_conversation[-1]["content"]
            })

    async def _call_llm(self, conversation: List[Dict[str, str]]) -> str:
        """LLM í˜¸ì¶œ - ì§ì ‘ HTTP API ì‚¬ìš©"""
        try:
            import httpx
            import json
            from orchestrator.config import LLM_CONFIG

            # ëŒ€í™” í˜•ì‹ ë³€í™˜
            api_messages = []
            for msg in conversation:
                api_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

            # HTTP API ì§ì ‘ í˜¸ì¶œ
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{LLM_CONFIG['base_url']}/chat/completions",
                    json={
                        "model": LLM_CONFIG["model"],
                        "messages": api_messages,
                        "stream": False,
                        "temperature": 0.7,
                        "max_tokens": 2000
                    },
                    headers={"Authorization": f"Bearer {LLM_CONFIG['api_key']}"} if LLM_CONFIG['api_key'] else {},
                    timeout=30.0
                )

                if response.status_code == 200:
                    result = response.json()
                    if 'choices' in result and len(result['choices']) > 0:
                        content = result['choices'][0]['message']['content']
                        return content.strip() if content else "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    else:
                        return "LLM ì‘ë‹µ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
                else:
                    print(f"LLM API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                    return "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"

        except Exception as e:
            print(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"

    async def _execute_tool(self, tool_call: ToolCall) -> Any:
        """íˆ´ ì‹¤í–‰"""
        if tool_call.name not in self.tools:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {tool_call.name}")

        tool = self.tools[tool_call.name]

        try:
            # MCP íˆ´ ì‹¤í–‰
            if hasattr(tool, 'call'):
                # ë¹„ë™ê¸° í˜¸ì¶œ
                if asyncio.iscoroutinefunction(tool.call):
                    result = await tool.call(tool_call.arguments)
                else:
                    result = tool.call(tool_call.arguments)
            else:
                # ë‹¤ë¥¸ í˜•íƒœì˜ íˆ´ ì¸í„°í˜ì´ìŠ¤ ì²˜ë¦¬
                result = await tool(tool_call.arguments)

            return result

        except Exception as e:
            raise Exception(f"ë„êµ¬ '{tool_call.name}' ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def reset_conversation(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []