import asyncio
from typing import List, Any, AsyncGenerator, Dict, Optional
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_core import CancellationToken
from autogen_agentchat.base import Response
from .tool_parser import ToolCallParser, ToolCall

class PromptBasedAgent:
    """í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ íˆ´ í˜¸ì¶œì„ ì²˜ë¦¬í•˜ëŠ” ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸"""

    def __init__(self, model_client, tools: List[Any], system_message: str, max_tool_iterations: int = 5):
        self.model_client = model_client
        self.tools = {tool.name: tool for tool in tools} if tools else {}
        self.system_message = system_message
        self.max_tool_iterations = max_tool_iterations
        self.conversation_history = []

    async def run(self, task: str) -> Response:
        """ê¸°ë³¸ ì‹¤í–‰ ë©”ì„œë“œ (AutoGen í˜¸í™˜ì„±)"""
        messages = []
        async for message in self.run_stream(task=task):
            messages.append(message)

        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ chat_messageë¡œ, ë‚˜ë¨¸ì§€ë¥¼ inner_messagesë¡œ ì‚¬ìš©
        if messages:
            return Response(chat_message=messages[-1], inner_messages=messages[:-1] if len(messages) > 1 else None)
        else:
            # ë¹ˆ ì‘ë‹µì˜ ê²½ìš°
            empty_message = TextMessage(content="No response generated.", source="assistant")
            return Response(chat_message=empty_message)

    async def run_stream(self, task: str) -> AsyncGenerator[ChatMessage, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        input_message = TextMessage(content=task, source="user")
        async for message in self.on_messages_stream([input_message], CancellationToken()):
            yield message

    async def on_messages_stream(self, messages: List[ChatMessage], cancellation_token: CancellationToken) -> AsyncGenerator[ChatMessage, None]:
        """ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ (AutoGen í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤)"""
        # ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        for msg in messages:
            if hasattr(msg, 'content') and hasattr(msg, 'source'):
                self.conversation_history.append({"role": msg.source, "content": msg.content})

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨í•œ ì „ì²´ ëŒ€í™” êµ¬ì„±
        full_conversation = [{"role": "system", "content": self.system_message}]
        full_conversation.extend(self.conversation_history)

        # íˆ´ í˜¸ì¶œ ë°˜ë³µ ì²˜ë¦¬
        iteration = 0
        while iteration < self.max_tool_iterations:
            if cancellation_token.is_cancelled():
                break

            # LLM í˜¸ì¶œ
            try:
                response = await self._call_llm(full_conversation)
                if not response:
                    break

                # íˆ´ í˜¸ì¶œ íŒŒì‹±
                tool_calls = ToolCallParser.parse_tool_calls(response)

                if not tool_calls:
                    # íˆ´ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ìµœì¢… ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬
                    clean_response = ToolCallParser.remove_tool_calls_from_response(response)
                    if clean_response.strip():
                        yield TextMessage(content=clean_response, source="assistant")
                    break

                # íˆ´ í˜¸ì¶œ ì‹¤í–‰
                tool_results = []
                for tool_call in tool_calls:
                    try:
                        result = await self._execute_tool(tool_call)
                        tool_results.append(ToolCallParser.format_tool_result(tool_call.name, result))

                        # íˆ´ ì‹¤í–‰ ìƒíƒœë¥¼ ìŠ¤íŠ¸ë¦¬ë°
                        status_msg = f"ğŸ›  ë„êµ¬ '{tool_call.name}' ì‹¤í–‰ ì™„ë£Œ"
                        yield TextMessage(content=status_msg, source="assistant")

                    except Exception as e:
                        error_result = f"ì˜¤ë¥˜: {str(e)}"
                        tool_results.append(ToolCallParser.format_tool_result(tool_call.name, error_result))

                # íˆ´ ê²°ê³¼ë¥¼ ëŒ€í™”ì— ì¶”ê°€
                if tool_results:
                    tool_response = response + "\n" + "\n".join(tool_results)
                    full_conversation.append({"role": "assistant", "content": tool_response})

                iteration += 1

            except Exception as e:
                error_msg = f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}"
                yield TextMessage(content=error_msg, source="assistant")
                break

        # ë§ˆì§€ë§‰ ëŒ€í™”ë¥¼ íˆìŠ¤í† ë¦¬ì— ì €ì¥
        if full_conversation and len(full_conversation) > len(self.conversation_history) + 1:
            self.conversation_history.append({
                "role": "assistant",
                "content": full_conversation[-1]["content"]
            })

    async def _call_llm(self, conversation: List[Dict[str, str]]) -> str:
        """LLM í˜¸ì¶œ"""
        try:
            # ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìœ„í•´ ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ì‚¬ìš©
            user_messages = [msg for msg in conversation if msg["role"] == "user"]
            if not user_messages:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ì‚¬ìš©
            system_msg = next((msg for msg in conversation if msg["role"] == "system"), None)
            last_user_msg = user_messages[-1]

            simple_messages = []
            if system_msg:
                simple_messages.append(TextMessage(content=system_msg["content"], source="system"))
            simple_messages.append(TextMessage(content=last_user_msg["content"], source="user"))

            # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            response_chunks = []
            try:
                async for chunk in self.model_client.create_stream(messages=simple_messages):
                    if hasattr(chunk, 'content') and chunk.content:
                        response_chunks.append(chunk.content)
            except Exception as stream_error:
                print(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {stream_error}")
                # í´ë°±: ê°„ë‹¨í•œ ì‘ë‹µ
                return "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"

            result = "".join(response_chunks)
            return result if result.strip() else "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        except Exception as e:
            print(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def _execute_tool(self, tool_call: ToolCall) -> Any:
        """íˆ´ ì‹¤í–‰"""
        if tool_call.name not in self.tools:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {tool_call.name}")

        tool = self.tools[tool_call.name]

        try:
            # MCP íˆ´ ì‹¤í–‰
            if hasattr(tool, 'call'):
                # ë¹„ë™ê¸° í˜¸ì¶œ
                if asyncio.iscoroutinefunction(tool.call):
                    result = await tool.call(tool_call.arguments)
                else:
                    result = tool.call(tool_call.arguments)
            else:
                # ë‹¤ë¥¸ í˜•íƒœì˜ íˆ´ ì¸í„°í˜ì´ìŠ¤ ì²˜ë¦¬
                result = await tool(tool_call.arguments)

            return result

        except Exception as e:
            raise Exception(f"ë„êµ¬ '{tool_call.name}' ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def reset_conversation(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []